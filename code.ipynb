{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17cc0ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# loads environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe52166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "# Make sure to set your OpenAI API key in the environment variable OPENAI_API_KEY\n",
    "# Alternatively, you can pass the key directly to the OpenAI constructor with `api_key='your_api_key'`\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101bc552",
   "metadata": {},
   "source": [
    "# Core function to query GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "736895af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_gpt(prompt):\n",
    "    \"\"\"\n",
    "    Queries the GPT model with a given prompt and returns the completion.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The input prompt to send to the GPT model\n",
    "        \n",
    "    Returns:\n",
    "        str: The model's response/completion text\n",
    "    \"\"\"\n",
    "    # Call the OpenAI API to get a completion\n",
    "    # Ensure you have the correct model and parameters set\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages = [\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': '',\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': prompt,\n",
    "            }\n",
    "\n",
    "        ],\n",
    "        model='gpt-4o',\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    completion = chat_completion.choices[0].message.content\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191f8daa",
   "metadata": {},
   "source": [
    "# Function to query other hugginface models (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0d2332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "def query_hf_model(model_name: str, prompt: str, max_new_tokens: int = 100, temperature: float = 0.7) -> str:\n",
    "    \"\"\"\n",
    "    Generate text from a Hugging Face language model.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Hugging Face model name (e.g., 'gpt2', 'meta-llama/Llama-2-7b-chat-hf').\n",
    "        prompt (str): Input prompt text.\n",
    "        max_new_tokens (int): Maximum number of tokens to generate.\n",
    "        temperature (float): Sampling temperature.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text.\n",
    "    \"\"\"\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize input\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    # Generate output\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id  # avoids warning for some models\n",
    "        )\n",
    "\n",
    "    # Decode and return only the newly generated part\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return generated_text[len(prompt):].strip()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "output = query_hf_model(\"gpt2\", \"Once upon a time\", max_new_tokens=50)\n",
    "print(\"Generated:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d3621d",
   "metadata": {},
   "source": [
    "# Study 1 Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12abf896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_prompt(interview_domain, interview_turns):\n",
    "    \"\"\"\n",
    "    Generates a prompt for generating follow-up questions based on interview context.\n",
    "    \n",
    "    Args:\n",
    "        interview_domain (str): The domain/topic of the interview\n",
    "        interview_turns (str): The conversation history between interviewer and interviewee\n",
    "        \n",
    "    Returns:\n",
    "        str: A formatted prompt for generating follow-up questions\n",
    "    \"\"\"\n",
    "    prompt = f\"You are an AI agent capable of generating context summaries.\\\n",
    "      During a requirements elicitation interview with an interviewee about how the interviewee conducts {interview_domain}, \\\n",
    "      the INTERVIEWEE and INTERVIEWER have had the following conversation: {interview_turns}. \\\n",
    "      Generate a follow-up question that the Interviewer should ask next based on the conversation. \\\n",
    "      Restrict your response to only show the follow-up question without explanation.\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7379e71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your CSV file containing the interview data\n",
    "# The default path to reproduce the results in the paper is '~/datasets/study1.csv'\n",
    "notebook_dir = os.getcwd()\n",
    "df = pd.read_csv(notebook_dir+ '/datasets/study1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68b7d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Interview Domain</th>\n",
       "      <th>Interview Turns</th>\n",
       "      <th>Human Follow Up Question</th>\n",
       "      <th>No. Of Relevant Speaker Turns</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apartment finding</td>\n",
       "      <td>Interviewer: What kind of things do you look f...</td>\n",
       "      <td>So, the three things you are looking for are t...</td>\n",
       "      <td>1</td>\n",
       "      <td>Confirmation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apartment finding</td>\n",
       "      <td>Interviewer: Okay, and you mentioned that pric...</td>\n",
       "      <td>You mentioned that you were looking for an apa...</td>\n",
       "      <td>1</td>\n",
       "      <td>Answer probing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apartment finding</td>\n",
       "      <td>Interviewer: Okay, so you would rank them as f...</td>\n",
       "      <td>Apart from not knowing the person, what other ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Answer probing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apartment finding</td>\n",
       "      <td>Interviewer: Right, and you also mentioned the...</td>\n",
       "      <td>Have you had any incident that required settling?</td>\n",
       "      <td>1</td>\n",
       "      <td>Answer probing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Apartment finding</td>\n",
       "      <td>Interviewer: What kind of things do you look f...</td>\n",
       "      <td>Okay, and the last thing that you mentioned th...</td>\n",
       "      <td>3</td>\n",
       "      <td>Answer probing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Interview Domain                                    Interview Turns  \\\n",
       "0  Apartment finding  Interviewer: What kind of things do you look f...   \n",
       "1  Apartment finding  Interviewer: Okay, and you mentioned that pric...   \n",
       "2  Apartment finding  Interviewer: Okay, so you would rank them as f...   \n",
       "3  Apartment finding  Interviewer: Right, and you also mentioned the...   \n",
       "4  Apartment finding  Interviewer: What kind of things do you look f...   \n",
       "\n",
       "                            Human Follow Up Question  \\\n",
       "0  So, the three things you are looking for are t...   \n",
       "1  You mentioned that you were looking for an apa...   \n",
       "2  Apart from not knowing the person, what other ...   \n",
       "3  Have you had any incident that required settling?   \n",
       "4  Okay, and the last thing that you mentioned th...   \n",
       "\n",
       "   No. Of Relevant Speaker Turns            Type  \n",
       "0                              1    Confirmation  \n",
       "1                              1  Answer probing  \n",
       "2                              1  Answer probing  \n",
       "3                              1  Answer probing  \n",
       "4                              3  Answer probing  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate the input dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7629f250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the dataset and generate questions\n",
    "\n",
    "print(\"Generating follow-up questions...\")\n",
    "for i in range(len(df)):\n",
    "    # Extract the interview domain and turns for the current row\n",
    "    interview_domain = df.iloc[i]['Interview Domain']\n",
    "    interview_turns = df.iloc[i]['Interview Turns']\n",
    "\n",
    "    # Generate the context prompt for the current interview turn\n",
    "    prompt = context_prompt(interview_domain, interview_turns)\n",
    "    # Query the GPT model with the generated prompt\n",
    "    response = query_gpt(prompt)\n",
    "\n",
    "    # Store the generated question in the DataFrame\n",
    "    df.at[i, 'LLM Follow Up Question'] = response\n",
    "    print(f\"Generated question for interview turn {i+1}: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e8eecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a new CSV file\n",
    "df2 = pd.DataFrame()\n",
    "df2['Interview Domain'] = df['Interview Domain']\n",
    "df2['Interview Turns'] = df['Interview Turns']\n",
    "df2['Human Follow-up Question'] = df['Human Follow Up Question']\n",
    "df2['LLM Follow-up Question'] = df['LLM Follow Up Question']\n",
    "df2.to_csv('study1_out.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5676ad2",
   "metadata": {},
   "source": [
    "# Study 2 & 3 Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17c80495",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion1 = \"A good follow-up question should elicit tacit \\\n",
    "assumptions, i.e. justify or authorize assumptions stakeholders tacitly made without justification.\"\n",
    "\n",
    "criterion2 = \"A good follow-up question should consider alternatives, \\\n",
    "i.e. look for alternative information or alternatives to existing requirements.\"\n",
    "\n",
    "criterion3 = \"A good follow-up question should clarify when unclear, \\\n",
    "i.e. ask for clarification whenever words INTERVIEWEE said are unclear. \\\n",
    "To classify whether the INTERVIEWER's question meets this standard, first consider \\\n",
    "if the INTERVIEWEE's speech contains unclear statements. If it does not contain anything \\\n",
    "unclear, then the standard is met. Otherwise, look at whether the INTERVIEWER's \\\n",
    "question tries to clarify the unclear. \"\n",
    "\n",
    "criterion4 = \"A good follow-up question should clarify when contradictory, \\\n",
    "i.e. ask for clarification whenever INTERVIEWEE mentioned two conflicting requirements or described \\\n",
    "potential features in conflicting terms. For example, if the INTERVIEWEE said he wanted to have an app \\\n",
    "recommending to him good restaurants based on his age and gender, but he also wanted the app to protect \\\n",
    "his private information, then this may be a case that needs clarification. \\\n",
    "To classify whether the INTERVIEWER's question meets this standard, first consider \\\n",
    "if the INTERVIEWEE mentioned anything contradictory. If it does not contain anything \\\n",
    "contradictory, then the standard is met. Otherwise, look at whether the INTERVIEWER's \\\n",
    "question tries to clarify the contradiction. \"\n",
    "\n",
    "criterion5 = \"A good follow-up question should elicit tacit knowledge, \\\n",
    "i.e. elicit tacit knowledge that are known to INTERVIEWEE but unknown to analysts.\"\n",
    "\n",
    "criterion6 = \"A good follow-up question should be related to the interview domain, \\\n",
    "and should not be too generic.\"\n",
    "\n",
    "criterion7 = \"A good follow-up question should not be too long or articulated, \\\n",
    "which would require the interviewee to ask for repeating or rephrase multiple times. \\\n",
    "To classify whether the INTERVIEWER's question meets this standard, first consider \\\n",
    "if the question is too long. If not, then the standard is met. Otherwise, the standard is not met.\"\n",
    "\n",
    "criterion8 = \"A good follow-up question should use common vocabulary, \\\n",
    "it should not contain special words or expressions that are not in the common vocabulary. \\\n",
    "To classify whether the INTERVIEWER's question meets this standard, first consider \\\n",
    "if the question contains any jargon. If not, then the standard is met. Otherwise, the standard is not met.\"\n",
    "\n",
    "criterion9 = \"A good follow-up question should not require technical knowledge in order to answer. \\\n",
    "For example, in a clinic finding interview, the question should not ask about the \\\n",
    "diagnosis criteria or data analysis. \\\n",
    "To classify whether the INTERVIEWER's question meets this standard, first consider \\\n",
    "if the INTERVIEWER's question contained anything that needs technical knowledge in order to understand \\\n",
    "or answer. If it does not contain anything \\\n",
    "technical, then the standard is met. Otherwise, the standard is not met.\"\n",
    "\n",
    "criterion10 = \"A good follow-up question should be \\\n",
    "appropriate to interviewee's profile, i.e., ask questions that can be answered \\\n",
    "by the interviewee given the interviewee's profile. For example, when conducting crowd-based interviews, \\\n",
    "it's inappropriate to ask crowd users about industry-specific topics such as software development cycles. \\\n",
    "To classify whether the INTERVIEWER's question meets this standard, \\\n",
    "first consider if the question contains any word inappropriate to crowd users. \\\n",
    "If the question contains no such word, then the standard is met. Otherwise, the standard is not met.\"\n",
    "\n",
    "criterion11 = \"A good follow-up question should not ask the interviewee to present a \\\n",
    "solution to satisfy a requirement. For example, it's inappropriate to ask users about how to design a specific \\\n",
    "feature, or what would an ideal user interface look like. To classify whether the INTERVIEWER's question meets this standard, \\\n",
    "first consider if the question is asking the INTERVIEWEE to give a solution. If not, then the standard is met. \\\n",
    "Otherwise, the standard is not met.\"\n",
    "\n",
    "criterion12 = \"A good follow-up question should not \\\n",
    "involve multiple kinds of requirements, i.e., it should not mix different categories of requirements \\\n",
    "or multiple specific requirements within one category into one single question. \\\n",
    "To classify whether the INTERVIEWER's question meets this standard, first consider if \\\n",
    "the question is asking for more than one requirement. If it contains only one kind of requirement, \\\n",
    "the standard is met. If more than one kind of requirement is involved, then the standard is not met.\"\n",
    "\n",
    "criterion13 = \"A good follow-up question should avoid asking questions \\\n",
    "that lead to multiple interpretations, which are questions that can be interpreted in more than one way. \\\n",
    "To classify whether the INTERVIEWER's question meets this standard, first consider if \\\n",
    "the question can be interpreted in more than one way. If not, the standard is met. \\\n",
    "Otherwise, the standard is not met. \"\n",
    "\n",
    "criterion14 = \"A good follow-up question should avoid asking vague questions \\\n",
    "that could infer no reasonable meaning, which are questions that do not have enough context or \\\n",
    "clarity for interviewee to answer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a375e1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# study 2 classification prompt\n",
    "def generate_prompt_classify(domain, interviewee_speech, interviewer_response, criterion):\n",
    "    \"\"\"\n",
    "    Generates a prompt for classifying whether an interviewer's response meets a specific criterion.\n",
    "    \n",
    "    Args:\n",
    "        domain (str): The domain/topic of the interview\n",
    "        interviewee_speech (str): What the interviewee said\n",
    "        interviewer_response (str): The interviewer's follow-up question\n",
    "        criterion (str): The specific criterion to evaluate against\n",
    "        \n",
    "    Returns:\n",
    "        str: A formatted prompt for classification\n",
    "    \"\"\"\n",
    "    prompt = f\"You are an AI agent capable of conducting requirements elicitation interviews.\\\n",
    " During a requirements elicitation interview with an interviewee about how the interviewee conducts {domain}, \\\n",
    "the INTERVIEWEE said '{interviewee_speech}'. Then the INTERVIEWER asked a follow up question \\\n",
    "by saying '{interviewer_response}'. Standard: {criterion} \\\n",
    "Please classify based solely on whether the INTERVIEWER’s response meets this specific standard, and \\\n",
    "refrain from using any other standards related to follow up questions when you classify. \\\n",
    "If the INTERVIEWER’s response meets this standard, output 'Yes', otherwise output 'No'. \\\n",
    "Restrict your response to output only 'Yes' or 'No' without explanations.\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c6ba54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# study 3 question generation prompt\n",
    "def generate_prompt_question(domain, interviewee_speech, interviewer_response, criterion):\n",
    "    \"\"\"\n",
    "    Generates a prompt for creating a follow-up question that meets a specific criterion.\n",
    "    \n",
    "    Args:\n",
    "        domain (str): The domain/topic of the interview\n",
    "        interviewee_speech (str): What the interviewee said\n",
    "        interviewer_response (str): The interviewer's follow-up question\n",
    "        criterion (str): The specific criterion the new question should meet\n",
    "        \n",
    "    Returns:\n",
    "        str: A formatted prompt for question generation\n",
    "    \"\"\"\n",
    "    prompt = f\"You are an AI agent capable of conducting requirements elicitation interviews.\\\n",
    " During a requirements elicitation interview with an interviewee about how the interviewee conducts {domain}, \\\n",
    "the INTERVIEWEE said '{interviewee_speech}'. Generate a follow-up question that meets the following \\\n",
    "criterion based ONLY on what the INTERVIEWEE said, and restrict your response to only show the \\\n",
    "follow-up question without explanation. Criterion: {criterion}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b64d55a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_responses_2_step(dataset_file):\n",
    "    \"\"\"\n",
    "    Processes a dataset to generate and classify follow-up questions based on multiple criteria.\n",
    "    Uses a two-step process: first classifies existing responses, then generates new ones if needed.\n",
    "    \n",
    "    Args:\n",
    "        dataset_file (str): Path to the CSV file containing the dataset\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Updated dataset with new responses and classifications\n",
    "    \"\"\"\n",
    "    dataset_df = pd.read_csv(dataset_file, dtype='object')\n",
    "    \n",
    "    for index, row in dataset_df.iterrows():\n",
    "        # Extracting the necessary fields from the dataset\n",
    "        interviewee_speech = row['Interviewee_Speech']\n",
    "        interviewer_response = row['Interviewer_Response']\n",
    "        domain = row['Domain']\n",
    "        \n",
    "        # Initialize the columns for criteria\n",
    "        criteria = [criterion1, criterion2, criterion3, criterion4, criterion5, criterion6, criterion7, criterion8, criterion9, criterion10, criterion11, criterion12, criterion13, criterion14]\n",
    "        column_names = list(dataset_df.columns[3:])\n",
    "        \n",
    "        # Iterate through each criterion to classify the interviewer's response\n",
    "        for i in range(len(criteria)):\n",
    "            criterion = criteria[i]\n",
    "            # classification prompt\n",
    "            prompt_c = generate_prompt_classify(domain, interviewee_speech, interviewer_response, criterion)\n",
    "            response = query_gpt(prompt_c)\n",
    "            \n",
    "            # If the response does not meet the criterion, generate a new question\n",
    "            if \"No\" in response:\n",
    "                # generation prompts\n",
    "                prompt_q = generate_prompt_question(domain, interviewee_speech, interviewer_response, criterion)\n",
    "                response_q = query_gpt(prompt_q)\n",
    "                dataset_df.at[index, column_names[i]] = response_q           \n",
    "    \n",
    "    return dataset_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0b5e677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your CSV file containing the interview data\n",
    "# The default path to reproduce the results in the paper is '~/datasets/study2.csv'\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "dataset_df = get_responses_2_step(notebook_dir + \"/datasets/study2.csv\")\n",
    "output_file = \"study2_out.csv\"\n",
    "dataset_df.to_csv(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63757abb",
   "metadata": {},
   "source": [
    "# Side Study: LLM to avoid all mistakes + self-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7169d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# side study prompt\n",
    "def generate_prompt_all_cri(domain, interviewee_speech, criteria_lst):\n",
    "    \"\"\"\n",
    "    Generates a prompt for creating a follow-up question that meets all specified criteria.\n",
    "    \n",
    "    Args:\n",
    "        domain (str): The domain/topic of the interview\n",
    "        interviewee_speech (str): What the interviewee said\n",
    "        criteria_lst (list): List of all criteria the question should meet\n",
    "        \n",
    "    Returns:\n",
    "        str: A formatted prompt for generating a question meeting all criteria\n",
    "    \"\"\"\n",
    "    prompt = f\"You are an AI agent capable of conducting requirements elicitation interviews.\\\n",
    " During a requirements elicitation interview with an interviewee about how the interviewee conducts {domain}, \\\n",
    "the INTERVIEWEE said '{interviewee_speech}'. Generate a follow-up question that meets ALL of the following \\\n",
    "criteria based ONLY on what the INTERVIEWEE said, and restrict your response to only show the \\\n",
    "follow-up question without explanation. \\\n",
    "Criterion 1: {criteria_lst[0]} \\\n",
    "Criterion 2: {criteria_lst[1]} \\\n",
    "Criterion 3: {criteria_lst[2]} \\\n",
    "Criterion 4: {criteria_lst[3]} \\\n",
    "Criterion 5: {criteria_lst[4]} \\\n",
    "Criterion 6: {criteria_lst[5]} \\\n",
    "Criterion 7: {criteria_lst[6]} \\\n",
    "Criterion 8: {criteria_lst[7]} \\\n",
    "Criterion 9: {criteria_lst[8]} \\\n",
    "Criterion 10: {criteria_lst[9]} \\\n",
    "Criterion 11: {criteria_lst[10]} \\\n",
    "Criterion 12: {criteria_lst[11]} \\\n",
    "Criterion 13: {criteria_lst[12]} \\\n",
    "Criterion 14: {criteria_lst[13]}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ed5e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_responses_all_cri(dataset_file):\n",
    "    \"\"\"\n",
    "    Processes a dataset to generate follow-up questions that meet all criteria.\n",
    "    \n",
    "    Args:\n",
    "        dataset_file (str): Path to the CSV file containing the dataset\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Updated dataset with new responses that meet all criteria\n",
    "    \"\"\"\n",
    "    dataset_df = pd.read_csv(dataset_file, dtype='object')\n",
    "    \n",
    "    for index, row in dataset_df.iterrows():\n",
    "        \n",
    "        interviewee_speech = row['Interviewee_Speech']\n",
    "        domain = row['Domain']\n",
    "        \n",
    "        criteria = [criterion1, criterion2, criterion3, criterion4, criterion5, criterion6, criterion7, criterion8, criterion9, criterion10, criterion11, criterion12, criterion13, criterion14]\n",
    "        \n",
    "        prompt_all_cri = generate_prompt_all_cri(domain, interviewee_speech, criteria)\n",
    "        response_all_cri = query_gpt(prompt_all_cri)\n",
    "                   \n",
    "        dataset_df.at[index, 'Interviewer_Response'] = response_all_cri\n",
    "    \n",
    "    return dataset_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01404689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your CSV file containing the interview data\n",
    "# The default path to reproduce the results in the paper is '~/datasets/study2.csv'\n",
    "\n",
    "dataset_all_cri = get_responses_all_cri(\"study2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae360b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"side_study.csv\"\n",
    "dataset_all_cri.to_csv(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c39cf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reclassify prompt\n",
    "def generate_prompt_classify(domain, interviewee_speech, interviewer_response, criterion):\n",
    "    prompt = f\"You are an AI agent capable of conducting requirements elicitation interviews.\\\n",
    " During a requirements elicitation interview with an interviewee about how the interviewee conducts {domain}, \\\n",
    "the INTERVIEWEE said '{interviewee_speech}'. Then the INTERVIEWER asked a follow up question \\\n",
    "by saying '{interviewer_response}'. Standard: {criterion} \\\n",
    "Please classify based solely on whether the INTERVIEWER’s response meets this specific standard, and \\\n",
    "refrain from using any other standards related to follow up questions when you classify. \\\n",
    "If the INTERVIEWER’s response meets this standard, output 'Yes', otherwise output 'No'. \\\n",
    "Restrict your response to output only 'Yes' or 'No' without explanations.\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bfa8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_responses_reclassify(dataset_file):\n",
    "    \"\"\"\n",
    "    Reclassifies responses in a dataset against all criteria.\n",
    "    \n",
    "    Args:\n",
    "        dataset_file (str): Path to the CSV file containing the dataset\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Updated dataset with new classifications for all criteria\n",
    "    \"\"\"\n",
    "    dataset_df = pd.read_csv(dataset_file, dtype='object')\n",
    "    \n",
    "    for index, row in dataset_df.iterrows():\n",
    "        \n",
    "        interviewee_speech = row['Interviewee_Speech']\n",
    "        interviewer_response = row['Interviewer_Response']\n",
    "        domain = row['Domain']\n",
    "        \n",
    "\n",
    "        criteria = [criterion1, criterion2, criterion3, criterion4, criterion5, criterion6, criterion7, criterion8, criterion9, criterion10, criterion11, criterion12, criterion13, criterion14]\n",
    "        column_names = list(dataset_df.columns[4:])\n",
    "        \n",
    "        for i in range(len(criteria)):\n",
    "            criterion = criteria[i]\n",
    "            # classification prompt\n",
    "            prompt_c = generate_prompt_classify(domain, interviewee_speech, interviewer_response, criterion)\n",
    "            response = query_gpt(prompt_c)\n",
    "            dataset_df.at[index, column_names[i]] = response           \n",
    "    \n",
    "    return dataset_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a0f332",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = get_responses_reclassify(\"side_study.csv\")\n",
    "output_file = \"side_study.csv\"\n",
    "dataset_df.to_csv(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54750b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
