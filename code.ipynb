{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cc0ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# loads environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe52166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPEN_AI_KEY = \"\" # fill in the API key\n",
    "client = OpenAI(api_key=OPEN_AI_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101bc552",
   "metadata": {},
   "source": [
    "# Core function to query GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736895af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_gpt(prompt):\n",
    "    \"\"\"\n",
    "    Queries the GPT model with a given prompt and returns the completion.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The input prompt to send to the GPT model\n",
    "        \n",
    "    Returns:\n",
    "        str: The model's response/completion text\n",
    "    \"\"\"\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages = [\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': '',\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': prompt,\n",
    "            }\n",
    "\n",
    "        ],\n",
    "        model='gpt-4o',\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    completion = chat_completion.choices[0].message.content\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e6f249",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6d3621d",
   "metadata": {},
   "source": [
    "# Study 1 Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12abf896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_prompt(interview_domain, interview_turns):\n",
    "    \"\"\"\n",
    "    Generates a prompt for generating follow-up questions based on interview context.\n",
    "    \n",
    "    Args:\n",
    "        interview_domain (str): The domain/topic of the interview\n",
    "        interview_turns (str): The conversation history between interviewer and interviewee\n",
    "        \n",
    "    Returns:\n",
    "        str: A formatted prompt for generating follow-up questions\n",
    "    \"\"\"\n",
    "    prompt = f\"You are an AI agent capable of generating context summaries.\\\n",
    "      During a requirements elicitation interview with an interviewee about how the interviewee conducts {interview_domain}, \\\n",
    "      the INTERVIEWEE and INTERVIEWER have had the following conversation: {interview_turns}. \\\n",
    "      Generate a follow-up question that the Interviewer should ask next based on the conversation. \\\n",
    "      Restrict your response to only show the follow-up question without explanation.\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7379e71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your CSV file containing the interview data\n",
    "df = pd.read_csv('path/to/your/file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7629f250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the dataset and generate questions\n",
    "for i in range(len(df)):\n",
    "    interview_domain = df.iloc[i]['Interview Domain']\n",
    "    interview_turns = df.iloc[i]['Interview Turns']\n",
    "    prompt = context_prompt(interview_domain, interview_turns)\n",
    "    response = query_gpt(prompt)\n",
    "    df.at[i, 'LLM Follow-up'] = response\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e8eecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save output to csv\n",
    "df2 = pd.DataFrame()\n",
    "df2['Interview Domain'] = df['Interview Domain']\n",
    "df2['Interview Turns'] = df['Interview Turns']\n",
    "df2['Human Follow-up Question'] = df['Human Follow-up']\n",
    "df2['LLM Follow-up Question'] = df['LLM Follow-up']\n",
    "df2.to_csv('study1_out.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5676ad2",
   "metadata": {},
   "source": [
    "# Study 2 & 3 Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c80495",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion1 = \"A good follow-up question should elicit tacit \\\n",
    "assumptions, i.e. justify or authorize assumptions stakeholders tacitly made without justification.\"\n",
    "\n",
    "criterion2 = \"A good follow-up question should consider alternatives, \\\n",
    "i.e. look for alternative information or alternatives to existing requirements.\"\n",
    "\n",
    "criterion3 = \"A good follow-up question should clarify when unclear, \\\n",
    "i.e. ask for clarification whenever words INTERVIEWEE said are unclear. \\\n",
    "To classify whether the INTERVIEWER's question meets this standard, first consider \\\n",
    "if the INTERVIEWEE's speech contains unclear statements. If it does not contain anything \\\n",
    "unclear, then the standard is met. Otherwise, look at whether the INTERVIEWER's \\\n",
    "question tries to clarify the unclear. \"\n",
    "\n",
    "criterion4 = \"A good follow-up question should clarify when contradictory, \\\n",
    "i.e. ask for clarification whenever INTERVIEWEE mentioned two conflicting requirements or described \\\n",
    "potential features in conflicting terms. For example, if the INTERVIEWEE said he wanted to have an app \\\n",
    "recommending to him good restaurants based on his age and gender, but he also wanted the app to protect \\\n",
    "his private information, then this may be a case that needs clarification. \\\n",
    "To classify whether the INTERVIEWER's question meets this standard, first consider \\\n",
    "if the INTERVIEWEE mentioned anything contradictory. If it does not contain anything \\\n",
    "contradictory, then the standard is met. Otherwise, look at whether the INTERVIEWER's \\\n",
    "question tries to clarify the contradiction. \"\n",
    "\n",
    "criterion5 = \"A good follow-up question should elicit tacit knowledge, \\\n",
    "i.e. elicit tacit knowledge that are known to INTERVIEWEE but unknown to analysts.\"\n",
    "\n",
    "criterion6 = \"A good follow-up question should be related to the interview domain, \\\n",
    "and should not be too generic.\"\n",
    "\n",
    "criterion7 = \"A good follow-up question should not be too long or articulated, \\\n",
    "which would require the interviewee to ask for repeating or rephrase multiple times. \\\n",
    "To classify whether the INTERVIEWER's question meets this standard, first consider \\\n",
    "if the question is too long. If not, then the standard is met. Otherwise, the standard is not met.\"\n",
    "\n",
    "criterion8 = \"A good follow-up question should use common vocabulary, \\\n",
    "it should not contain special words or expressions that are not in the common vocabulary. \\\n",
    "To classify whether the INTERVIEWER's question meets this standard, first consider \\\n",
    "if the question contains any jargon. If not, then the standard is met. Otherwise, the standard is not met.\"\n",
    "\n",
    "criterion9 = \"A good follow-up question should not require technical knowledge in order to answer. \\\n",
    "For example, in a clinic finding interview, the question should not ask about the \\\n",
    "diagnosis criteria or data analysis. \\\n",
    "To classify whether the INTERVIEWER's question meets this standard, first consider \\\n",
    "if the INTERVIEWER's question contained anything that needs technical knowledge in order to understand \\\n",
    "or answer. If it does not contain anything \\\n",
    "technical, then the standard is met. Otherwise, the standard is not met.\"\n",
    "\n",
    "criterion10 = \"A good follow-up question should be \\\n",
    "appropriate to interviewee's profile, i.e., ask questions that can be answered \\\n",
    "by the interviewee given the interviewee's profile. For example, when conducting crowd-based interviews, \\\n",
    "it's inappropriate to ask crowd users about industry-specific topics such as software development cycles. \\\n",
    "To classify whether the INTERVIEWER's question meets this standard, \\\n",
    "first consider if the question contains any word inappropriate to crowd users. \\\n",
    "If the question contains no such word, then the standard is met. Otherwise, the standard is not met.\"\n",
    "\n",
    "criterion11 = \"A good follow-up question should not ask the interviewee to present a \\\n",
    "solution to satisfy a requirement. For example, it's inappropriate to ask users about how to design a specific \\\n",
    "feature, or what would an ideal user interface look like. To classify whether the INTERVIEWER's question meets this standard, \\\n",
    "first consider if the question is asking the INTERVIEWEE to give a solution. If not, then the standard is met. \\\n",
    "Otherwise, the standard is not met.\"\n",
    "\n",
    "criterion12 = \"A good follow-up question should not \\\n",
    "involve multiple kinds of requirements, i.e., it should not mix different categories of requirements \\\n",
    "or multiple specific requirements within one category into one single question. \\\n",
    "To classify whether the INTERVIEWER's question meets this standard, first consider if \\\n",
    "the question is asking for more than one requirement. If it contains only one kind of requirement, \\\n",
    "the standard is met. If more than one kind of requirement is involved, then the standard is not met.\"\n",
    "\n",
    "criterion13 = \"A good follow-up question should avoid asking questions \\\n",
    "that lead to multiple interpretations, which are questions that can be interpreted in more than one way. \\\n",
    "To classify whether the INTERVIEWER's question meets this standard, first consider if \\\n",
    "the question can be interpreted in more than one way. If not, the standard is met. \\\n",
    "Otherwise, the standard is not met. \"\n",
    "\n",
    "criterion14 = \"A good follow-up question should avoid asking vague questions \\\n",
    "that could infer no reasonable meaning, which are questions that do not have enough context or \\\n",
    "clarity for interviewee to answer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a375e1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# study 2 classification prompt\n",
    "def generate_prompt_classify(domain, interviewee_speech, interviewer_response, criterion):\n",
    "    \"\"\"\n",
    "    Generates a prompt for classifying whether an interviewer's response meets a specific criterion.\n",
    "    \n",
    "    Args:\n",
    "        domain (str): The domain/topic of the interview\n",
    "        interviewee_speech (str): What the interviewee said\n",
    "        interviewer_response (str): The interviewer's follow-up question\n",
    "        criterion (str): The specific criterion to evaluate against\n",
    "        \n",
    "    Returns:\n",
    "        str: A formatted prompt for classification\n",
    "    \"\"\"\n",
    "    prompt = f\"You are an AI agent capable of conducting requirements elicitation interviews.\\\n",
    " During a requirements elicitation interview with an interviewee about how the interviewee conducts {domain}, \\\n",
    "the INTERVIEWEE said '{interviewee_speech}'. Then the INTERVIEWER asked a follow up question \\\n",
    "by saying '{interviewer_response}'. Standard: {criterion} \\\n",
    "Please classify based solely on whether the INTERVIEWER’s response meets this specific standard, and \\\n",
    "refrain from using any other standards related to follow up questions when you classify. \\\n",
    "If the INTERVIEWER’s response meets this standard, output 'Yes', otherwise output 'No'. \\\n",
    "Restrict your response to output only 'Yes' or 'No' without explanations.\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6ba54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# study 3 question generation prompt\n",
    "def generate_prompt_question(domain, interviewee_speech, interviewer_response, criterion):\n",
    "    \"\"\"\n",
    "    Generates a prompt for creating a follow-up question that meets a specific criterion.\n",
    "    \n",
    "    Args:\n",
    "        domain (str): The domain/topic of the interview\n",
    "        interviewee_speech (str): What the interviewee said\n",
    "        interviewer_response (str): The interviewer's follow-up question\n",
    "        criterion (str): The specific criterion the new question should meet\n",
    "        \n",
    "    Returns:\n",
    "        str: A formatted prompt for question generation\n",
    "    \"\"\"\n",
    "    prompt = f\"You are an AI agent capable of conducting requirements elicitation interviews.\\\n",
    " During a requirements elicitation interview with an interviewee about how the interviewee conducts {domain}, \\\n",
    "the INTERVIEWEE said '{interviewee_speech}'. Generate a follow-up question that meets the following \\\n",
    "criterion based ONLY on what the INTERVIEWEE said, and restrict your response to only show the \\\n",
    "follow-up question without explanation. Criterion: {criterion}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64d55a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_responses_2_step(dataset_file):\n",
    "    \"\"\"\n",
    "    Processes a dataset to generate and classify follow-up questions based on multiple criteria.\n",
    "    Uses a two-step process: first classifies existing responses, then generates new ones if needed.\n",
    "    \n",
    "    Args:\n",
    "        dataset_file (str): Path to the CSV file containing the dataset\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Updated dataset with new responses and classifications\n",
    "    \"\"\"\n",
    "    dataset_df = pd.read_csv(dataset_file, dtype='object')\n",
    "    \n",
    "    for index, row in dataset_df.iterrows():\n",
    "        \n",
    "        interviewee_speech = row['Interviewee_Speech']\n",
    "        interviewer_response = row['Interviewer_Response']\n",
    "        domain = row['Domain']\n",
    "        \n",
    "\n",
    "        criteria = [criterion1, criterion2, criterion3, criterion4, criterion5, criterion6, criterion7, criterion8, criterion9, criterion10, criterion11, criterion12, criterion13, criterion14]\n",
    "        column_names = list(dataset_df.columns[3:])\n",
    "        \n",
    "        for i in range(len(criteria)):\n",
    "            criterion = criteria[i]\n",
    "            # classification prompt\n",
    "            prompt_c = generate_prompt_classify(domain, interviewee_speech, interviewer_response, criterion)\n",
    "            response = query_gpt(prompt_c)\n",
    "            \n",
    "            if \"No\" in response:\n",
    "                # generation prompts\n",
    "                prompt_q = generate_prompt_question(domain, interviewee_speech, interviewer_response, criterion)\n",
    "                response_q = query_gpt(prompt_q)\n",
    "                dataset_df.at[index, column_names[i]] = response_q           \n",
    "    \n",
    "    return dataset_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b5e677",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = get_responses_2_step(\"study2.csv\")\n",
    "output_file = \"study2_out.csv\"\n",
    "dataset_df.to_csv(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63757abb",
   "metadata": {},
   "source": [
    "# Side Study: LLM to avoid all mistakes + self-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7169d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# side study prompt\n",
    "def generate_prompt_all_cri(domain, interviewee_speech, criteria_lst):\n",
    "    \"\"\"\n",
    "    Generates a prompt for creating a follow-up question that meets all specified criteria.\n",
    "    \n",
    "    Args:\n",
    "        domain (str): The domain/topic of the interview\n",
    "        interviewee_speech (str): What the interviewee said\n",
    "        criteria_lst (list): List of all criteria the question should meet\n",
    "        \n",
    "    Returns:\n",
    "        str: A formatted prompt for generating a question meeting all criteria\n",
    "    \"\"\"\n",
    "    prompt = f\"You are an AI agent capable of conducting requirements elicitation interviews.\\\n",
    " During a requirements elicitation interview with an interviewee about how the interviewee conducts {domain}, \\\n",
    "the INTERVIEWEE said '{interviewee_speech}'. Generate a follow-up question that meets ALL of the following \\\n",
    "criteria based ONLY on what the INTERVIEWEE said, and restrict your response to only show the \\\n",
    "follow-up question without explanation. \\\n",
    "Criterion 1: {criteria_lst[0]} \\\n",
    "Criterion 2: {criteria_lst[1]} \\\n",
    "Criterion 3: {criteria_lst[2]} \\\n",
    "Criterion 4: {criteria_lst[3]} \\\n",
    "Criterion 5: {criteria_lst[4]} \\\n",
    "Criterion 6: {criteria_lst[5]} \\\n",
    "Criterion 7: {criteria_lst[6]} \\\n",
    "Criterion 8: {criteria_lst[7]} \\\n",
    "Criterion 9: {criteria_lst[8]} \\\n",
    "Criterion 10: {criteria_lst[9]} \\\n",
    "Criterion 11: {criteria_lst[10]} \\\n",
    "Criterion 12: {criteria_lst[11]} \\\n",
    "Criterion 13: {criteria_lst[12]} \\\n",
    "Criterion 14: {criteria_lst[13]}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ed5e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_responses_all_cri(dataset_file):\n",
    "    \"\"\"\n",
    "    Processes a dataset to generate follow-up questions that meet all criteria.\n",
    "    \n",
    "    Args:\n",
    "        dataset_file (str): Path to the CSV file containing the dataset\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Updated dataset with new responses that meet all criteria\n",
    "    \"\"\"\n",
    "    dataset_df = pd.read_csv(dataset_file, dtype='object')\n",
    "    \n",
    "    for index, row in dataset_df.iterrows():\n",
    "        \n",
    "        interviewee_speech = row['Interviewee_Speech']\n",
    "        domain = row['Domain']\n",
    "        \n",
    "        criteria = [criterion1, criterion2, criterion3, criterion4, criterion5, criterion6, criterion7, criterion8, criterion9, criterion10, criterion11, criterion12, criterion13, criterion14]\n",
    "        \n",
    "        prompt_all_cri = generate_prompt_all_cri(domain, interviewee_speech, criteria)\n",
    "        response_all_cri = query_gpt(prompt_all_cri)\n",
    "                   \n",
    "        dataset_df.at[index, 'Interviewer_Response'] = response_all_cri\n",
    "    \n",
    "    return dataset_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01404689",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_all_cri = get_responses_all_cri(\"study2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae360b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"side_study.csv\"\n",
    "dataset_all_cri.to_csv(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c39cf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reclassify prompt\n",
    "def generate_prompt_classify(domain, interviewee_speech, interviewer_response, criterion):\n",
    "    prompt = f\"You are an AI agent capable of conducting requirements elicitation interviews.\\\n",
    " During a requirements elicitation interview with an interviewee about how the interviewee conducts {domain}, \\\n",
    "the INTERVIEWEE said '{interviewee_speech}'. Then the INTERVIEWER asked a follow up question \\\n",
    "by saying '{interviewer_response}'. Standard: {criterion} \\\n",
    "Please classify based solely on whether the INTERVIEWER’s response meets this specific standard, and \\\n",
    "refrain from using any other standards related to follow up questions when you classify. \\\n",
    "If the INTERVIEWER’s response meets this standard, output 'Yes', otherwise output 'No'. \\\n",
    "Restrict your response to output only 'Yes' or 'No' without explanations.\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bfa8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_responses_reclassify(dataset_file):\n",
    "    \"\"\"\n",
    "    Reclassifies responses in a dataset against all criteria.\n",
    "    \n",
    "    Args:\n",
    "        dataset_file (str): Path to the CSV file containing the dataset\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Updated dataset with new classifications for all criteria\n",
    "    \"\"\"\n",
    "    dataset_df = pd.read_csv(dataset_file, dtype='object')\n",
    "    \n",
    "    for index, row in dataset_df.iterrows():\n",
    "        \n",
    "        interviewee_speech = row['Interviewee_Speech']\n",
    "        interviewer_response = row['Interviewer_Response']\n",
    "        domain = row['Domain']\n",
    "        \n",
    "\n",
    "        criteria = [criterion1, criterion2, criterion3, criterion4, criterion5, criterion6, criterion7, criterion8, criterion9, criterion10, criterion11, criterion12, criterion13, criterion14]\n",
    "        column_names = list(dataset_df.columns[4:])\n",
    "        \n",
    "        for i in range(len(criteria)):\n",
    "            criterion = criteria[i]\n",
    "            # classification prompt\n",
    "            prompt_c = generate_prompt_classify(domain, interviewee_speech, interviewer_response, criterion)\n",
    "            response = query_gpt(prompt_c)\n",
    "            dataset_df.at[index, column_names[i]] = response           \n",
    "    \n",
    "    return dataset_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a0f332",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = get_responses_reclassify(\"side_study.csv\")\n",
    "output_file = \"side_study.csv\"\n",
    "dataset_df.to_csv(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54750b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
